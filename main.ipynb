{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import constants\n",
    "from models import CnnTDFLstm1Dv2\n",
    "\n",
    "# from models import *\n",
    "from modelsv2 import *\n",
    "\n",
    "\n",
    "# utils\n",
    "\n",
    "def one_hot_e(dna_seq: str) -> np.ndarray:\n",
    "  mydict = {'A': np.asarray([1.0, 0.0, 0.0, 0.0]), 'C': np.asarray([0.0, 1.0, 0.0, 0.0]),\n",
    "            'G': np.asarray([0.0, 0.0, 1.0, 0.0]), 'T': np.asarray([0.0, 0.0, 0.0, 1.0]),\n",
    "            'N': np.asarray([0.0, 0.0, 0.0, 0.0]), 'H': np.asarray([0.0, 0.0, 0.0, 0.0]),\n",
    "            'a': np.asarray([1.0, 0.0, 0.0, 0.0]), 'c': np.asarray([0.0, 1.0, 0.0, 0.0]),\n",
    "            'g': np.asarray([0.0, 0.0, 1.0, 0.0]), 't': np.asarray([0.0, 0.0, 0.0, 1.0]),\n",
    "            'n': np.asarray([0.0, 0.0, 0.0, 0.0]), '-': np.asarray([0.0, 0.0, 0.0, 0.0])}\n",
    "\n",
    "  size_of_a_seq: int = len(dna_seq)\n",
    "\n",
    "  # forward = np.zeros(shape=(size_of_a_seq, 4))\n",
    "\n",
    "  forward_list: list = [mydict[dna_seq[i]] for i in range(0, size_of_a_seq)]\n",
    "  encoded = np.asarray(forward_list)\n",
    "  return encoded\n",
    "\n",
    "\n",
    "def one_hot_e_column(column: pd.Series) -> np.ndarray:\n",
    "  tmp_list: list = [one_hot_e(seq) for seq in column]\n",
    "  encoded_column = np.asarray(tmp_list)\n",
    "  return encoded_column\n",
    "\n",
    "\n",
    "def reverse_dna_seq(dna_seq: str) -> str:\n",
    "  # m_reversed = \"\"\n",
    "  # for i in range(0, len(dna_seq)):\n",
    "  #     m_reversed = dna_seq[i] + m_reversed\n",
    "  # return m_reversed\n",
    "  return dna_seq[::-1]\n",
    "\n",
    "\n",
    "def complement_dna_seq(dna_seq: str) -> str:\n",
    "  comp_map = {\"A\": \"T\", \"C\": \"G\", \"T\": \"A\", \"G\": \"C\",\n",
    "              \"a\": \"t\", \"c\": \"g\", \"t\": \"a\", \"g\": \"c\",\n",
    "              \"N\": \"N\", \"H\": \"H\", \"-\": \"-\",\n",
    "              \"n\": \"n\", \"h\": \"h\"\n",
    "              }\n",
    "\n",
    "  comp_dna_seq_list: list = [comp_map[nucleotide] for nucleotide in dna_seq]\n",
    "  comp_dna_seq: str = \"\".join(comp_dna_seq_list)\n",
    "  return comp_dna_seq\n",
    "\n",
    "\n",
    "def reverse_complement_dna_seq(dna_seq: str) -> str:\n",
    "  return reverse_dna_seq(complement_dna_seq(dna_seq))\n",
    "\n",
    "\n",
    "def reverse_complement_dna_seqs(column: pd.Series) -> pd.Series:\n",
    "  tmp_list: list = [reverse_complement_dna_seq(seq) for seq in column]\n",
    "  rc_column = pd.Series(tmp_list)\n",
    "  return rc_column\n",
    "\n",
    "#\n",
    "# class CNN1D(nn.Module):\n",
    "#   def __init__(self,\n",
    "#                in_channel_num_of_nucleotides=4,\n",
    "#                kernel_size_k_mer_motif=4,\n",
    "#                dnn_size=256,\n",
    "#                num_filters=1,\n",
    "#                lstm_hidden_size=128,\n",
    "#                *args, **kwargs):\n",
    "#     super().__init__(*args, **kwargs)\n",
    "#     self.conv1d = nn.Conv1d(in_channels=in_channel_num_of_nucleotides, out_channels=num_filters,\n",
    "#                             kernel_size=kernel_size_k_mer_motif, stride=2)\n",
    "#     self.activation = nn.ReLU()\n",
    "#     self.pooling = nn.MaxPool1d(kernel_size=kernel_size_k_mer_motif, stride=2)\n",
    "#\n",
    "#     self.flatten = nn.Flatten()\n",
    "#     # linear layer\n",
    "#\n",
    "#     self.dnn2 = nn.Linear(in_features=14 * num_filters, out_features=dnn_size)\n",
    "#     self.act2 = nn.Sigmoid()\n",
    "#     self.dropout2 = nn.Dropout(p=0.2)\n",
    "#\n",
    "#     self.out = nn.Linear(in_features=dnn_size, out_features=1)\n",
    "#     self.out_act = nn.Sigmoid()\n",
    "#\n",
    "#     pass\n",
    "#\n",
    "#   def forward(self, x):\n",
    "#     timber.debug(constants.magenta + f\"h0: {x}\")\n",
    "#     h = self.conv1d(x)\n",
    "#     timber.debug(constants.green + f\"h1: {h}\")\n",
    "#     h = self.activation(h)\n",
    "#     timber.debug(constants.magenta + f\"h2: {h}\")\n",
    "#     h = self.pooling(h)\n",
    "#     timber.debug(constants.blue + f\"h3: {h}\")\n",
    "#     timber.debug(constants.cyan + f\"h4: {h}\")\n",
    "#\n",
    "#     h = self.flatten(h)\n",
    "#     timber.debug(constants.magenta + f\"h5: {h},\\n shape {h.shape}, size {h.size}\")\n",
    "#     h = self.dnn2(h)\n",
    "#     timber.debug(constants.green + f\"h6: {h}\")\n",
    "#\n",
    "#     h = self.act2(h)\n",
    "#     timber.debug(constants.blue + f\"h7: {h}\")\n",
    "#\n",
    "#     h = self.dropout2(h)\n",
    "#     timber.debug(constants.cyan + f\"h8: {h}\")\n",
    "#\n",
    "#     h = self.out(h)\n",
    "#     timber.debug(constants.magenta + f\"h9: {h}\")\n",
    "#\n",
    "#     h = self.out_act(h)\n",
    "#     timber.debug(constants.green + f\"h10: {h}\")\n",
    "#     # h = (h > 0.5).float()  # <---- should this go here?\n",
    "#     # timber.debug(constants.green + f\"h11: {h}\")\n",
    "#\n",
    "#     return h\n",
    "#\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataframe):\n",
    "    self.x = dataframe[\"Sequence\"]\n",
    "    self.y = dataframe[\"class\"]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "\n",
    "  def preprocessing(self, x1, y1) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "    forward_col = x1\n",
    "\n",
    "    backward_col = reverse_complement_dna_seqs(forward_col)\n",
    "\n",
    "    forward_one_hot_e_col: np.ndarray = one_hot_e_column(forward_col)\n",
    "    backward_one_hot_e_col: np.ndarray = one_hot_e_column(backward_col)\n",
    "\n",
    "    tr_xf_tensor = torch.Tensor(forward_one_hot_e_col).permute(1, 2, 0)\n",
    "    tr_xb_tensor = torch.Tensor(backward_one_hot_e_col).permute(1, 2, 0)\n",
    "    # timber.debug(f\"y1 {y1}\")\n",
    "    tr_y1 = np.array([y1])  # <--- need to put it inside brackets\n",
    "\n",
    "    return tr_xf_tensor, tr_xb_tensor, tr_y1\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    m_seq = self.x.iloc[idx]\n",
    "    labels = self.y.iloc[idx]\n",
    "    xf, xb, y = self.preprocessing(m_seq, labels)\n",
    "    timber.debug(f\"xf -> {xf.shape}, xb -> {xb.shape}, y -> {y}\")\n",
    "    return xf, xb, y\n",
    "\n",
    "\n",
    "def test_dataloader():\n",
    "  df = pd.read_csv(\"todo.csv\")\n",
    "  X = df[\"Sequence\"]\n",
    "  y = df[\"class\"]\n",
    "\n",
    "  ds = CustomDataset(df)\n",
    "  loader = DataLoader(ds, shuffle=True, batch_size=16)\n",
    "\n",
    "  train_loader = loader\n",
    "\n",
    "  for data in train_loader:\n",
    "    timber.debug(data)\n",
    "    # xf, xb, y = data[0], data[1], data[2]\n",
    "    # timber.debug(f\"xf -> {xf.shape}, xb -> {xb.shape}, y -> {y.shape}\")\n",
    "  pass\n",
    "\n",
    "\n",
    "def get_callbacks() -> list:\n",
    "  # metric.auc ( uses trapezoidal rule) gave an error: x is neither increasing, nor decreasing. so I had to remove it\n",
    "  return [\n",
    "    (\"tr_acc\", EpochScoring(\n",
    "      metrics.accuracy_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=True,\n",
    "      name=\"train_acc\",\n",
    "    )),\n",
    "\n",
    "    (\"tr_recall\", EpochScoring(\n",
    "      metrics.recall_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=True,\n",
    "      name=\"train_recall\",\n",
    "    )),\n",
    "    (\"tr_precision\", EpochScoring(\n",
    "      metrics.precision_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=True,\n",
    "      name=\"train_precision\",\n",
    "    )),\n",
    "    (\"tr_roc_auc\", EpochScoring(\n",
    "      metrics.roc_auc_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"tr_auc\"\n",
    "    )),\n",
    "    (\"tr_f1\", EpochScoring(\n",
    "      metrics.f1_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"tr_f1\"\n",
    "    )),\n",
    "    # (\"valid_acc1\", EpochScoring(\n",
    "    #   metrics.accuracy_score,\n",
    "    #   lower_is_better=False,\n",
    "    #   on_train=False,\n",
    "    #   name=\"valid_acc1\",\n",
    "    # )),\n",
    "    (\"valid_recall\", EpochScoring(\n",
    "      metrics.recall_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"valid_recall\",\n",
    "    )),\n",
    "    (\"valid_precision\", EpochScoring(\n",
    "      metrics.precision_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"valid_precision\",\n",
    "    )),\n",
    "    (\"valid_roc_auc\", EpochScoring(\n",
    "      metrics.roc_auc_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"valid_auc\"\n",
    "    )),\n",
    "    (\"valid_f1\", EpochScoring(\n",
    "      metrics.f1_score,\n",
    "      lower_is_better=False,\n",
    "      on_train=False,\n",
    "      name=\"valid_f1\"\n",
    "    ))\n",
    "  ]\n",
    "\n",
    "\n",
    "def start():\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = CnnTDFLstm1Dv2().to(device) # get_stackoverflow_model().to(device)\n",
    "  # df = pd.read_csv(\"data64.csv\")  # use this line\n",
    "  df = pd.read_csv(\"data2000random.csv\")\n",
    "\n",
    "\n",
    "\n",
    "  X = df[\"Sequence\"]\n",
    "  y = df[\"class\"]\n",
    "\n",
    "  npa = np.array([y.values])\n",
    "\n",
    "  torch_tensor = torch.tensor(npa)  # [0, 1, 1, 0, ... ... ] a simple list\n",
    "  print(f\"torch_tensor: {torch_tensor}\")\n",
    "  # need to transpose it!\n",
    "\n",
    "  yt = torch.transpose(torch_tensor, 0, 1)\n",
    "\n",
    "  ds = CustomDataset(df)\n",
    "  loader = DataLoader(ds, shuffle=True)\n",
    "\n",
    "  # train_loader = loader\n",
    "  # test_loader = loader  # todo: load another dataset later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # model = CnnLstm1DNoBatchNormV2().to(device) # get_stackoverflow_model().to(device)\n",
    "  m_criterion = nn.BCEWithLogitsLoss\n",
    "  # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  m_optimizer = optim.Adam\n",
    "\n",
    "  net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=50,\n",
    "    criterion=m_criterion,\n",
    "    optimizer=m_optimizer,\n",
    "    lr=0.01,\n",
    "    # decay=0.01,\n",
    "    # momentum=0.9,\n",
    "\n",
    "    device=device,\n",
    "    classes=[\"no_mqtl\", \"yes_mqtl\"],\n",
    "    verbose=True,\n",
    "    callbacks=get_callbacks()\n",
    "  )\n",
    "\n",
    "  ohe_c = one_hot_e_column(X)\n",
    "  print(f\"ohe_c shape {ohe_c.shape}\")\n",
    "  ohe_c = torch.Tensor(ohe_c)\n",
    "  ohe_c = ohe_c.permute(0, 2, 1)\n",
    "  ohe_c = ohe_c.to(device)\n",
    "  print(f\"ohe_c shape {ohe_c.shape}\")\n",
    "\n",
    "  net.fit(X=ohe_c, y=yt)\n",
    "  y_proba = net.predict_proba(ohe_c)\n",
    "  # timber.info(f\"y_proba = {y_proba}\")\n",
    "  pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  start()\n",
    "  # test_dataloader()\n",
    "  pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
